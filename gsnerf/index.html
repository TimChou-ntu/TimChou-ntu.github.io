<!DOCTYPE html>
<html>
<head>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <meta charset="utf-8">
  <meta name="description"
        content="GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding">
  <meta name="keywords" content="NeRF, Semantic Segmentation, 3D computer vision">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ntu.jpeg">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://timchou-ntu.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding</h1>
          
          <h2 class="title is-size-3 publication-title">CVPR 2024</h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://timchou-ntu.github.io/">Zi-Ting Chou</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/shengyuhuang">Sheng-Yu Huang</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://jack24658735.github.io/">I-Jieh Liu</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://vllab.ee.ntu.edu.tw/ycwang.html">Yu-Chiang Frank Wang</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Taiwan University,</span>
            <span class="author-block"><sup>2</sup>NVIDIA</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.03608"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2403.03608"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/TimChou-ntu/GSNeRF"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero">
  <div class="container is-max-desktop">
      <!-- <div class="content has-text-justified">
      <center><img src="./static/images/fig1_for_project_page.png" width="65%"></center>  
        <p> -->
          <!-- <iframe src="fig1.pdf" style="width: 100%;height: 100%;border: none;"></iframe> -->
          <!-- <b>Challenges in multi-label federated learning.</b>
          Since diverse label correlations can be observed across clients, aggregating local models might not be sufficiently generalizable.
        </p> -->
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/watch?v=HiAsGOYdo68"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Utilizing multi-view inputs to synthesize novel-view images, Neural Radiance Fields (NeRF) have emerged as a popular research topic in 3D vision. In this work, we introduce a Generalizable Semantic Neural Radiance Field (GSNeRF), which uniquely takes image semantics into the synthesis process so that both novel view images and the associated semantic maps can be produced for unseen scenes. Our GSNeRF is composed of two stages: Semantic Geo-Reasoning and Depth-Guided Visual rendering. The former is able to observe multi-view image inputs to extract semantic and geometry features from a scene. Guided by the resulting image geometry information, the latter performs both image and semantic rendering with improved performances. Our experiments not only confirm that GSNeRF performs favorably against prior works on both novel-view image and semantic segmentation synthesis but the effectiveness of our sampling strategy for visual rendering is further verified.          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Framework</h2>
        <center><img src="./static/images/Framework.png" width="100%"></center>

        <div class="content has-text-justified">
          <p>
            <b>Overview of GSNeRF.</b>
            Given multi-view images of a scene, the Semantic Geo-Reasoner $ùê∫_ùúÉ$ predicts the depth map of each image, which is aggregated to estimate the target view depth map. With DT as key geometric guidance, we design Depth-Guided Visual Rendering to render target view image and segmentation map respectively.

          </p>
        </div>
        
        <!--/ Interpolating. -->

        <!-- <div class="columns is-centered"> -->
          <!-- Visual Effects. -->
          <!-- <div class="column is-full-width"> -->
            <div class="content">
              <h2 class="title is-3">Model Details</h2>
              <div class="content has-text-justified">
                <p>
                  While most methods using multi-view stereo to extract features of a scene, we predict depth from these features and further create two depth-guided sampling strategies to customize on two tasks.
                </p>
              </div>
              <br>
              <br>
              <div class="image-container">
                <img src="./static/images/reasoner.png" alt="Reasoner Image">
                <img src="./static/images/depth-guided-rendering.png" alt="Depth Guided Rendering Image">
              </div>
            </div>
          <!-- </div> -->
          <!--/ Visual Effects. -->

          <!-- <div class="column is-full-width"> -->
            <div class="content">
              <h2 class="title is-3">Algorithm</h2>
              <div class="content has-text-justified">
                <p>
                  We perform novel-view depth estimation by aggregating depth maps from multi-view images, which is simply projecting all the depth maps to 3D space then reprojecting to the target view. The depth-guided visual rendering is designed to render both image and segmentation map with the guidance of depth map.
                </p>
              </div>
              <center><img src="./static/images/alg.png" width="85%"></center>
            </div>
          <!-- </div> -->
          
        <!-- </div> -->
        <h3 class="title is-3">Experiments</h3>

        <div class="content has-text-justified">
          <center><img src="./static/images/main_table.png" width="75%"></center>
          <p>
            Table 1: Quantitative results on ScanNet & Replica. Note that methods in the first four rows take GT depth as inputs or training supervision, while the methods in the last six rows do not observe GT depth during training/testing.
          </p>
          <center><img src="./static/images/finetune.png" width="75%"></center>
          <p>
            Table 2: Results of finetuning on unseen scenes of ScanNet.
          </p>
        </div>
       
      </div>
    </div>
    <!--/ Animation. -->


  </div>
</section>


<section class="section" id="BibTeX">
 
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
      If you find this useful for your research, please consider citing:
    <pre><code>@inproceedings{Chou2024gsnerf,
      author    = {Zi‚ÄëTing Chou* and Sheng‚ÄëYu Huang* and I‚ÄëJieh Liu and Yu‚ÄëChiang Frank Wang},
      title     = {GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding},
      booktitle = CVPR,
      year      = {2024},
      selected  = {true},
      arxiv       = {2403.03608},
      bibtex_show = {true},
    }</code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://timchou-ntu.github.io/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This webpage template is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0 License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
